{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52cac1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score, KFold,cross_validate\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2be3000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    KNeighborsClassifier(),\n",
    "    LogisticRegression(solver='liblinear'),\n",
    "    DecisionTreeClassifier(),\n",
    "    BaggingClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    XGBClassifier(use_label_encoder=False,eval_metric=\"logloss\")\n",
    "]\n",
    "\n",
    "algorithm_names = np.array([\n",
    "    \"Knn\",\n",
    "    \"Lr\",\n",
    "    \"Dt\",\n",
    "    \"Bc\",\n",
    "    \"Rf\",\n",
    "    \"AdaB\",\n",
    "    \"Gb\",\n",
    "    \"XGB\"\n",
    "])\n",
    "\n",
    "\n",
    "metrics = [\n",
    "    'accuracy',\n",
    "    'roc_auc',\n",
    "]\n",
    "\n",
    "params = {\n",
    "    'Knn': {\n",
    "        'n_neighbors' : [3,10,15]\n",
    "    },\n",
    "    \n",
    "    'Dt': {\n",
    "        'max_depth': [2,4]\n",
    "    },\n",
    "    \n",
    "    'Gb' : {'learning_rate': [0.1,0.3,1]\n",
    "}}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc533a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Judge:\n",
    "    \n",
    "    def __init__(self,dataframe_name):\n",
    "        self.dataframe_name = dataframe_name\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.models = None\n",
    "        self.algorithm_names = None\n",
    "        self.params = None\n",
    "        self.metrics = None\n",
    "        return None\n",
    "        \n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\" Judging {self.dataframe_name}\"\n",
    "        \n",
    "    \n",
    "    def set_data(self,X,y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        return self\n",
    "    \n",
    "    def set_algorithms_and_names(self,models,algorithm_names):\n",
    "        self.models = models\n",
    "        self.algorithm_names = algorithm_names\n",
    "        return self\n",
    "    \n",
    "    def set_params(self, params):\n",
    "        self.params = params\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def set_metrics(self, metrics):\n",
    "        self.metrics = metrics\n",
    "        return self\n",
    "    \n",
    "        \n",
    "    @staticmethod\n",
    "    def Class_info(cls):\n",
    "        class_info = f\"\"\"Judge class contains methods for optimizing machine learning \n",
    "        performance and metrics evaluation for binary classification. It uses nested cross validation.\n",
    "        \n",
    "        Methods: \n",
    "        set_data -> to introduce data. X for the independent variables, y for the target. \n",
    "        set_algorithm_and_names -> to introduce different models (list) and their names (array). \n",
    "        set_params -> giving parameters for optimizing performances (dictionary) \n",
    "        set_metrics -> to introduce metrics to evaluate\n",
    "        get_final_performance -> to start the analysis \n",
    "        Hyperparameters get_final_performance:\n",
    "        cv_inner_splits: inner cross validation  \n",
    "        cv_outer_splits: outer cross validation \n",
    "        metric_to_optimize:\n",
    "        metric to optimize during parameter tuning (default: roc_auc) \n",
    "        find_params: set algorithm for parameters search -> GridSearchCV or RandomizedSearchCV(\n",
    "        default: GridSearchCV)\n",
    "        Returns:\n",
    "        Metric performance tab for each model (Dataframe) \"\"\"\n",
    "        \n",
    "        return class_info\n",
    "    \n",
    "    \n",
    "    def __Construct_matrix_df(self,algorithm_names,scores):\n",
    "    #Costruisco il dataframe a partire \n",
    "        Compacted_perf_values = np.hsplit(scores,len(self.metrics)) \n",
    "        Performance_matrix = np.append(algorithm_names,Compacted_perf_values).reshape(len(self.metrics)+1,len(self.models)).T \n",
    "        columns_vector = np.insert(self.metrics,0,\"model\") \n",
    "        Performance_matrix_df = pd.DataFrame(data=Performance_matrix,columns=columns_vector)\n",
    "        return Performance_matrix_df\n",
    "    \n",
    "    def __get_performance_from_algorithm(self, algorithm, grid, X, y, metrics,inner_cv,outer_cv,find_params,metric_to_optimize):\n",
    "        \n",
    "        if grid == {}:\n",
    "            cvl = cross_validate(algorithm, X, y, scoring = metrics,cv=self.outer_cv)\n",
    "            results = np.array(list(cvl.values()))[2::,:]\n",
    "        \n",
    "        else:\n",
    "            if self.find_params == \"GridSearchCV\":\n",
    "                clf = GridSearchCV(estimator = algorithm, param_grid = grid, scoring = metrics,refit=metric_to_optimize,cv=self.inner_cv)\n",
    "            elif self.find_params == \"RandomizedSearchCV\":\n",
    "                clf = RandomizedSearchCV(estimator = algorithm, param_distributions= grid, scoring = metrics,refit=metric_to_optimize,cv=self.inner_cv)\n",
    "            cvl = cross_validate(clf, X, y, scoring = metrics,cv=self.outer_cv)\n",
    "            results = np.array(list(cvl.values()))[2::,:]\n",
    "        \n",
    "        results = np.mean(results,axis=1)\n",
    "        return np.round(results*100,2)\n",
    "    \n",
    "    def get_final_performance(self,cv_inner_splits,cv_outer_splits,metric_to_optimize = \"roc_auc\",find_params=\"GridSearchCV\"):\n",
    "        self.inner_cv = KFold(n_splits=cv_inner_splits,shuffle=True)\n",
    "        self.outer_cv = KFold(n_splits=cv_outer_splits,shuffle=True)\n",
    "        self.find_params = find_params\n",
    "        self.metric_to_optimize = metric_to_optimize\n",
    "        self.scores = np.zeros((len(self.models),len(self.metrics))) #Initializing scoring matrix\n",
    "        for model in range(len(self.models)):\n",
    "            grid = {}\n",
    "            if self.algorithm_names[model] in self.params.keys(): #Sets grid for parameters\n",
    "                grid = self.params[self.algorithm_names[model]]\n",
    "            \n",
    "            score = self.__get_performance_from_algorithm(self.models[model],grid,self.X,self.y,self.metrics,self.inner_cv,self.outer_cv,self.find_params,self.metric_to_optimize) \n",
    "            self.scores[model] = score \n",
    "        \n",
    "        \n",
    "        Performance_matrix_df = self.__Construct_matrix_df(self.algorithm_names,self.scores)\n",
    "        return Performance_matrix_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6528ec40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset loading \n",
    "X, y = load_breast_cancer(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc8a9c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inizializing\n",
    "judge = Judge(\"Breast_Cancer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28a1c2eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Judge at 0x1ebdb860910>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "judge.set_data(X,y).set_algorithms_and_names(models,algorithm_names).set_params(params).set_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f987400f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Knn</td>\n",
       "      <td>92.97</td>\n",
       "      <td>96.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lr</td>\n",
       "      <td>95.26</td>\n",
       "      <td>99.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dt</td>\n",
       "      <td>92.27</td>\n",
       "      <td>94.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bc</td>\n",
       "      <td>94.56</td>\n",
       "      <td>97.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rf</td>\n",
       "      <td>96.13</td>\n",
       "      <td>99.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AdaB</td>\n",
       "      <td>95.79</td>\n",
       "      <td>98.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gb</td>\n",
       "      <td>96.66</td>\n",
       "      <td>99.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>XGB</td>\n",
       "      <td>97.02</td>\n",
       "      <td>99.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model accuracy roc_auc\n",
       "0   Knn    92.97   96.56\n",
       "1    Lr    95.26   99.29\n",
       "2    Dt    92.27   94.17\n",
       "3    Bc    94.56   97.72\n",
       "4    Rf    96.13   99.11\n",
       "5  AdaB    95.79    98.7\n",
       "6    Gb    96.66   99.36\n",
       "7   XGB    97.02   99.36"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Analysis finally!!\n",
    "judge.get_final_performance(5,5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
